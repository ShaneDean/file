**The BSD Packet Filter: A New Architecture for User-level Packet Capture**The Network Tap**BPF有两个主要的概念**network tap       从网络设备驱动中收集packets的拷贝，并把他们出输给监听程序packet filter             如果某个packet应当被接受，它将决定这个packet要复制多少份给监听的应用![image](https://github.com/ShaneDean/file/blob/master/blog/linux/bpf-overview.png?raw=true)图一 阐述了BPF接口的基本工作流程当一个packed到达网络接口时，链路层的设备驱动通常会发送到系统协议栈中。但是当BPF正在监听这个接口，这个需求将会先调用BPF。 用户定义的鼓励去确定是否一个包将被接受和每个包的多少字节应当被保存。对于每个接受packet的filter而言，BPF拷贝请求的大量数据给filter关联的buffer。然后设备驱动重新获得控制。如果packet不是本地主机的地址，驱动将会返回一个中断。其他情况下，正常的协议处理机进程继续工作自从进程可能需要看一下在网络上的每个packet，并且在两个packed的间隔时间可能就few microsseconds ，这样的话就不可能对每个packet去触发一个系统调用read，BPF就必须在几个packet中间收集数据，并且当monitoring程序需要的read的时候返回。To maintain packet boundaries，BPF encapsulate每个packet头中的一些数据，比如time stamp,length,and offsets for data alignmentPacket  Filtering由于网络监控器只是网络traffic中很小的一个部分，令人惊奇的性能优势在过滤不想要的interrupt context中的packets过程中得到了体现。如何最小化的内存消耗是现代工作站中主要的瓶颈。packet should be filtered "in place"(eg where the network interface DMA engine put it) 而不是拷贝到其他的kernel buffer中区。因此如果packet被接受了，only those bytes that were needed by the filtering process are referenced by the host相反的案例，SunOS 就是拷贝packet在filtering之前，这样直接导致性能下降。（接下来就是SunOs STREAMS NIT机制的详细过程说明）Tap Performance Measurements在讨论packet filter细节之前，我们需要提出一些测量的指标。这些性能指标需要独立于packet filter机制。我们配置了BPF 和NIT(上文所提及) 在SunOS内核4.1.1中，并运行在Sparcstation2 工作中中。the measurements reflect the overhead incurred during the interrupt prcocessing.(how long it takes each system to stash the packet into a buffer)。对于BPF 我们简单地两区在调用bpf_tap()的前后的时间，使用Sparcstation的microsecond clock。对于NIT 我测量调用snit_intr()前后的时间并加上拷贝promiscuous packet到 mbufs的时间。（promiscuous packet指的是那些没有本地主机地址的）。换句话说我们包含了NIT在适当地方拿取not filtering packets的细嫩那个干扰。为了获取精确的时间，在 instrumented code segments时，我们锁住了系统终端。测试的数据集是一个处理不同packetlength的矩阵。我们约定处理每个packet使用2个配置，'accept all' filter,和 'reject all' filter中间的实验对比 略过 The Filter Model假设有一个用户关系buffering model的设计，